{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import jieba\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "TENCENT_VECTOR_PATH = '/Users/huan/Desktop/Tencent_AILab_ChineseEmbedding/Tencent_AILab_ChineseEmbedding.txt'\n",
    "MAX_VOCAB_SIZE = 50000\n",
    "MAX_SEQUENCE_LENGTH = 20\n",
    "EMBEDDING_DIM = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['sentence']\n",
    "Y = data['label']\n",
    "null_filter = X.notnull() & Y.notnull()\n",
    "X = X.loc[null_filter]\n",
    "Y = Y.loc[null_filter]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size = 0.2, stratify = Y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of word considered: 17390\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words = MAX_VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(X)\n",
    "num_words = min(MAX_VOCAB_SIZE, len(tokenizer.word_index) + 1)\n",
    "print('Number of word considered: {}'.format(num_words))\n",
    "word2index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "padded_sequences = pad_sequences(sequences,maxlen = MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "embeddings_index = dict()\n",
    "with open(TENCENT_VECTOR_PATH,'r', encoding = 'utf-8') as f:\n",
    "    next(f)\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        if word in word2index:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    \n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, ind in word2index.items():\n",
    "    if ind < MAX_VOCAB_SIZE:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[ind] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input,Dense, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding,concatenate,Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(num_words,\n",
    "                    EMBEDDING_DIM,\n",
    "                    weights = [embedding_matrix],\n",
    "                    input_length = MAX_SEQUENCE_LENGTH,\n",
    "                    trainable = False)\n",
    "\n",
    "inputs = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "embedded_sequences = Embedding(input_dim=num_words, output_dim=200, input_length=MAX_SEQUENCE_LENGTH, weights=[embedding_matrix],trainable=False)(inputs)\n",
    "l_conv01 = Conv1D(128, 3,activation='relu',padding='same')(embedded_sequences)\n",
    "l_pool01 = MaxPooling1D(3)(l_conv01)\n",
    "l_conv02 = Conv1D(128, 4,activation='relu',padding='same')(embedded_sequences)\n",
    "l_pool02 = MaxPooling1D(5)(l_conv02)\n",
    "l_conv03 = Conv1D(128, 5,activation='relu',padding='same')(embedded_sequences)\n",
    "l_pool03 = MaxPooling1D(5)(l_conv03)\n",
    "l_merge = concatenate([l_pool01,l_pool02,l_pool03],axis=1)\n",
    "\n",
    "l_cov2 = Conv1D(128, 3, activation='relu',padding='same')(l_merge)\n",
    "l_pool2 = MaxPooling1D(10)(l_cov2)\n",
    "l_pool2 = Flatten()(l_pool2)\n",
    "l_dense = Dense(128, activation='relu')(l_pool2)\n",
    "encoder = Model(inputs, l_dense)\n",
    "\n",
    "encoded_vector = encoder(inputs)\n",
    "\n",
    "preds = Dense(2, activation='softmax')(encoded_vector)\n",
    "\n",
    "model=Model(inputs,preds)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "38/38 [==============================] - 2s 35ms/step - loss: 0.1510 - accuracy: 0.9255 - val_loss: 0.0349 - val_accuracy: 0.9866\n",
      "Epoch 2/10\n",
      "38/38 [==============================] - 1s 30ms/step - loss: 0.0158 - accuracy: 0.9948 - val_loss: 0.0252 - val_accuracy: 0.9900\n",
      "Epoch 3/10\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.0048 - accuracy: 0.9987 - val_loss: 0.0214 - val_accuracy: 0.9933\n",
      "Epoch 4/10\n",
      "38/38 [==============================] - 1s 30ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.0217 - val_accuracy: 0.9933\n",
      "Epoch 5/10\n",
      "38/38 [==============================] - 1s 30ms/step - loss: 7.2524e-04 - accuracy: 0.9998 - val_loss: 0.0237 - val_accuracy: 0.9942\n",
      "Epoch 6/10\n",
      "38/38 [==============================] - 1s 30ms/step - loss: 4.3061e-04 - accuracy: 0.9998 - val_loss: 0.0255 - val_accuracy: 0.9933\n",
      "Epoch 7/10\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 3.4104e-04 - accuracy: 0.9998 - val_loss: 0.0264 - val_accuracy: 0.9933\n",
      "Epoch 8/10\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 2.8402e-04 - accuracy: 0.9998 - val_loss: 0.0278 - val_accuracy: 0.9925\n",
      "Epoch 9/10\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 2.4740e-04 - accuracy: 0.9998 - val_loss: 0.0279 - val_accuracy: 0.9933\n",
      "Epoch 10/10\n",
      "38/38 [==============================] - 1s 32ms/step - loss: 2.2289e-04 - accuracy: 0.9998 - val_loss: 0.0285 - val_accuracy: 0.9942\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff93c7e0d90>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_sequences, Y, batch_size =128, epochs = 10,validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 还是要深度学习模型哟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentence(sentence):\n",
    "    x = tokenizer.texts_to_sequences([' '.join(list(jieba.cut(sentence)))])\n",
    "    x_pad= pad_sequences(x,maxlen = MAX_SEQUENCE_LENGTH)\n",
    "    return model.predict(x_pad)[:,1][0] > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentence('小婊子真骚')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentence('做爱了太爽了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentence('今天天气不错')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentence('军火走私是犯法的')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentence('新疆独立么')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
